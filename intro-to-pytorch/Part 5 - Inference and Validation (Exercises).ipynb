{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Validation\n",
    "\n",
    "Now that you have a trained network, you can use it for making predictions. This is typically called **inference**, a term borrowed from statistics. However, neural networks have a tendency to perform *too well* on the training data and aren't able to generalize to data that hasn't been seen before. This is called **overfitting** and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the **validation** set. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training. In this notebook, I'll show you how to do this in PyTorch. \n",
    "\n",
    "As usual, let's start by loading the dataset through torchvision. You'll learn more about torchvision and loading data in a later part. This time we'll be taking advantage of the test set which you can get by setting `train=False` here:\n",
    "\n",
    "```python\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "```\n",
    "\n",
    "The test set contains images just like the training set. Typically you'll see 10-20% of the original dataset held out for testing and validation with the rest being used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/data/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/data/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll create a model like normal, using the same one from my solution for part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        if self.use_gpu:\n",
    "            self.device = torch.device('cuda:0')\n",
    "            self.cuda(device=self.device)\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        if self.use_gpu:\n",
    "            x = x.to(self.device)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)) and top-5 error rate. We'll focus on accuracy here. First I'll do a forward pass with one batch from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "# Get the class probabilities\n",
    "ps = torch.exp(model(images))\n",
    "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probabilities, we can get the most likely class using the `ps.topk` method. This returns the $k$ highest values. Since we just want the most likely class, we can use `ps.topk(1)`. This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7],\n",
      "        [7],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7],\n",
      "        [7],\n",
      "        [6],\n",
      "        [7],\n",
      "        [6],\n",
      "        [7]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if the predicted classes match the labels. This is simple to do by equating `top_class` and `labels`, but we have to be careful of the shapes. Here `top_class` is a 2D tensor with shape `(64, 1)` while `labels` is 1D with shape `(64)`. To get the equality to work out the way we want, `top_class` and `labels` must have the same shape.\n",
    "\n",
    "If we do\n",
    "\n",
    "```python\n",
    "equals = top_class == labels\n",
    "```\n",
    "\n",
    "`equals` will have shape `(64, 64)`, try it yourself. What it's doing is comparing the one element in each row of `top_class` with each element in `labels` which returns 64 True/False boolean values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "equals = top_class == labels.to(model.device).view(*top_class.shape)\n",
    "print(equals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the percentage of correct predictions. `equals` has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to `torch.mean`. If only it was that simple. If you try `torch.mean(equals)`, you'll get an error\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor\n",
    "```\n",
    "\n",
    "This happens because `equals` has type `torch.ByteTensor` but `torch.mean` isn't implement for tensors with that type. So we'll need to convert `equals` to a float tensor. Note that when we take `torch.mean` it returns a scalar tensor, to get the actual value as a float we'll need to do `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 12.5%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is untrained so it's making random guesses and we should see an accuracy around 10%. Now let's train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we're not updating our parameters in the validation pass, we can speed up our code by turning off gradients using `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```\n",
    "\n",
    ">**Exercise:** Implement the validation loop below and print out the total accuracy after the loop. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you'll always learn more by typing it rather than copy-pasting. You should be able to get an accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "[Epoch 0] Training Loss = 0.8603152987932918 (6336/60000)\n",
      "[Epoch 0] Training Loss = 0.7066635044675377 (12736/60000)\n",
      "[Epoch 0] Training Loss = 0.6345847812185319 (19136/60000)\n",
      "[Epoch 0] Training Loss = 0.5951401565158576 (25536/60000)\n",
      "[Epoch 0] Training Loss = 0.5703202819597267 (31936/60000)\n",
      "[Epoch 0] Training Loss = 0.5524306642840024 (38336/60000)\n",
      "[Epoch 0] Training Loss = 0.5362203903336382 (44736/60000)\n",
      "[Epoch 0] Training Loss = 0.5226830918999279 (51136/60000)\n",
      "[Epoch 0] Training Loss = 0.5118180936051158 (57536/60000)\n",
      "[Epoch 0] Training Accuracy: 52.078891257995735\n",
      "[Epoch 0] Test Accuracy: 53.76433121019108\n",
      "[Epoch 0] Test Loss: 0.44203739599057823\n",
      "############################################################\n",
      "[Epoch 1] Training Loss = 0.4016636841826969 (6336/60000)\n",
      "[Epoch 1] Training Loss = 0.40158332869335633 (12736/60000)\n",
      "[Epoch 1] Training Loss = 0.3971323968225897 (19136/60000)\n",
      "[Epoch 1] Training Loss = 0.3976211951751458 (25536/60000)\n",
      "[Epoch 1] Training Loss = 0.40091400922001724 (31936/60000)\n",
      "[Epoch 1] Training Loss = 0.3980061864489705 (38336/60000)\n",
      "[Epoch 1] Training Loss = 0.3956243291667773 (44736/60000)\n",
      "[Epoch 1] Training Loss = 0.39347182543428433 (51136/60000)\n",
      "[Epoch 1] Training Loss = 0.3910732676458438 (57536/60000)\n",
      "[Epoch 1] Training Accuracy: 54.8773987206823\n",
      "[Epoch 1] Test Accuracy: 54.388535031847134\n",
      "[Epoch 1] Test Loss: 0.4089373801923861\n",
      "############################################################\n",
      "[Epoch 2] Training Loss = 0.3469624772216334 (6336/60000)\n",
      "[Epoch 2] Training Loss = 0.3479566918545632 (12736/60000)\n",
      "[Epoch 2] Training Loss = 0.3451728826282814 (19136/60000)\n",
      "[Epoch 2] Training Loss = 0.34642889513108965 (25536/60000)\n",
      "[Epoch 2] Training Loss = 0.3487270884290487 (31936/60000)\n",
      "[Epoch 2] Training Loss = 0.3498690135665051 (38336/60000)\n",
      "[Epoch 2] Training Loss = 0.35004780366910204 (44736/60000)\n",
      "[Epoch 2] Training Loss = 0.35095818724030997 (51136/60000)\n",
      "[Epoch 2] Training Loss = 0.35223038337950713 (57536/60000)\n",
      "[Epoch 2] Training Accuracy: 55.61087420042644\n",
      "[Epoch 2] Test Accuracy: 54.847133757961785\n",
      "[Epoch 2] Test Loss: 0.39382227410556403\n",
      "############################################################\n",
      "[Epoch 3] Training Loss = 0.33193536810200625 (6336/60000)\n",
      "[Epoch 3] Training Loss = 0.3348843113860892 (12736/60000)\n",
      "[Epoch 3] Training Loss = 0.33119104089944257 (19136/60000)\n",
      "[Epoch 3] Training Loss = 0.3327955022491608 (25536/60000)\n",
      "[Epoch 3] Training Loss = 0.330115220724103 (31936/60000)\n",
      "[Epoch 3] Training Loss = 0.3302826543955851 (38336/60000)\n",
      "[Epoch 3] Training Loss = 0.32802607503870185 (44736/60000)\n",
      "[Epoch 3] Training Loss = 0.3278125120846739 (51136/60000)\n",
      "[Epoch 3] Training Loss = 0.3282854491150684 (57536/60000)\n",
      "[Epoch 3] Training Accuracy: 56.272921108742004\n",
      "[Epoch 3] Test Accuracy: 54.70700636942675\n",
      "[Epoch 3] Test Loss: 0.38722031976387\n",
      "############################################################\n",
      "[Epoch 4] Training Loss = 0.320702699850304 (6336/60000)\n",
      "[Epoch 4] Training Loss = 0.31557158728939805 (12736/60000)\n",
      "[Epoch 4] Training Loss = 0.3218508270073894 (19136/60000)\n",
      "[Epoch 4] Training Loss = 0.32294583559633794 (25536/60000)\n",
      "[Epoch 4] Training Loss = 0.3222526091641797 (31936/60000)\n",
      "[Epoch 4] Training Loss = 0.31911603219570817 (38336/60000)\n",
      "[Epoch 4] Training Loss = 0.3183873551064977 (44736/60000)\n",
      "[Epoch 4] Training Loss = 0.31625502807401745 (51136/60000)\n",
      "[Epoch 4] Training Loss = 0.3146905504722351 (57536/60000)\n",
      "[Epoch 4] Training Accuracy: 56.64179104477612\n",
      "[Epoch 4] Test Accuracy: 54.05095541401274\n",
      "[Epoch 4] Test Loss: 0.44056684936687446\n",
      "############################################################\n",
      "[Epoch 5] Training Loss = 0.3168080067544272 (6336/60000)\n",
      "[Epoch 5] Training Loss = 0.30851097184060206 (12736/60000)\n",
      "[Epoch 5] Training Loss = 0.3020217597085895 (19136/60000)\n",
      "[Epoch 5] Training Loss = 0.2995732947250357 (25536/60000)\n",
      "[Epoch 5] Training Loss = 0.3009574960969493 (31936/60000)\n",
      "[Epoch 5] Training Loss = 0.3010569800419083 (38336/60000)\n",
      "[Epoch 5] Training Loss = 0.3000158492758209 (44736/60000)\n",
      "[Epoch 5] Training Loss = 0.3003346869659066 (51136/60000)\n",
      "[Epoch 5] Training Loss = 0.30109237348277523 (57536/60000)\n",
      "[Epoch 5] Training Accuracy: 56.85820895522388\n",
      "[Epoch 5] Test Accuracy: 55.29299363057325\n",
      "[Epoch 5] Test Loss: 0.3659229897864305\n",
      "############################################################\n",
      "[Epoch 6] Training Loss = 0.2819936586299328 (6336/60000)\n",
      "[Epoch 6] Training Loss = 0.2864008287913236 (12736/60000)\n",
      "[Epoch 6] Training Loss = 0.28481212975986825 (19136/60000)\n",
      "[Epoch 6] Training Loss = 0.28557953794946644 (25536/60000)\n",
      "[Epoch 6] Training Loss = 0.28591880948844556 (31936/60000)\n",
      "[Epoch 6] Training Loss = 0.2839304036732906 (38336/60000)\n",
      "[Epoch 6] Training Loss = 0.2847182917087545 (44736/60000)\n",
      "[Epoch 6] Training Loss = 0.2889090010684184 (51136/60000)\n",
      "[Epoch 6] Training Loss = 0.2894012401081431 (57536/60000)\n",
      "[Epoch 6] Training Accuracy: 57.078891257995735\n",
      "[Epoch 6] Test Accuracy: 55.56687898089172\n",
      "[Epoch 6] Test Loss: 0.362406103711599\n",
      "############################################################\n",
      "[Epoch 7] Training Loss = 0.26923088021952696 (6336/60000)\n",
      "[Epoch 7] Training Loss = 0.27773344906131225 (12736/60000)\n",
      "[Epoch 7] Training Loss = 0.2740112519184483 (19136/60000)\n",
      "[Epoch 7] Training Loss = 0.27207361262543756 (25536/60000)\n",
      "[Epoch 7] Training Loss = 0.2723971489137543 (31936/60000)\n",
      "[Epoch 7] Training Loss = 0.27331924142443476 (38336/60000)\n",
      "[Epoch 7] Training Loss = 0.2763720502456029 (44736/60000)\n",
      "[Epoch 7] Training Loss = 0.2778446460993627 (51136/60000)\n",
      "[Epoch 7] Training Loss = 0.2782039792091483 (57536/60000)\n",
      "[Epoch 7] Training Accuracy: 57.427505330490405\n",
      "[Epoch 7] Test Accuracy: 55.796178343949045\n",
      "[Epoch 7] Test Loss: 0.34792651971624156\n",
      "############################################################\n",
      "[Epoch 8] Training Loss = 0.25850736684720926 (6336/60000)\n",
      "[Epoch 8] Training Loss = 0.26301345938638826 (12736/60000)\n",
      "[Epoch 8] Training Loss = 0.26012014309847636 (19136/60000)\n",
      "[Epoch 8] Training Loss = 0.25986174657418015 (25536/60000)\n",
      "[Epoch 8] Training Loss = 0.26159204806169434 (31936/60000)\n",
      "[Epoch 8] Training Loss = 0.2644023224400459 (38336/60000)\n",
      "[Epoch 8] Training Loss = 0.263665771199782 (44736/60000)\n",
      "[Epoch 8] Training Loss = 0.264042711894824 (51136/60000)\n",
      "[Epoch 8] Training Loss = 0.26604347015928903 (57536/60000)\n",
      "[Epoch 8] Training Accuracy: 57.578891257995735\n",
      "[Epoch 8] Test Accuracy: 55.62420382165605\n",
      "[Epoch 8] Test Loss: 0.3774337787537058\n",
      "############################################################\n",
      "[Epoch 9] Training Loss = 0.23810159964392882 (6336/60000)\n",
      "[Epoch 9] Training Loss = 0.25813652425255607 (12736/60000)\n",
      "[Epoch 9] Training Loss = 0.25439003802661514 (19136/60000)\n",
      "[Epoch 9] Training Loss = 0.2573477766268833 (25536/60000)\n",
      "[Epoch 9] Training Loss = 0.2594721727894399 (31936/60000)\n",
      "[Epoch 9] Training Loss = 0.2596488074587661 (38336/60000)\n",
      "[Epoch 9] Training Loss = 0.26110304143807406 (44736/60000)\n",
      "[Epoch 9] Training Loss = 0.25926414917273277 (51136/60000)\n",
      "[Epoch 9] Training Loss = 0.2617581299235744 (57536/60000)\n",
      "[Epoch 9] Training Accuracy: 57.6865671641791\n",
      "[Epoch 9] Test Accuracy: 55.65605095541401\n",
      "[Epoch 9] Test Loss: 0.35879583557130423\n",
      "############################################################\n",
      "[Epoch 10] Training Loss = 0.2323491374651591 (6336/60000)\n",
      "[Epoch 10] Training Loss = 0.2478986863964167 (12736/60000)\n",
      "[Epoch 10] Training Loss = 0.25070115039280827 (19136/60000)\n",
      "[Epoch 10] Training Loss = 0.24952849845651695 (25536/60000)\n",
      "[Epoch 10] Training Loss = 0.25406806089566325 (31936/60000)\n",
      "[Epoch 10] Training Loss = 0.2562722222621433 (38336/60000)\n",
      "[Epoch 10] Training Loss = 0.2550705038970376 (44736/60000)\n",
      "[Epoch 10] Training Loss = 0.2545413081679908 (51136/60000)\n",
      "[Epoch 10] Training Loss = 0.253880182494543 (57536/60000)\n",
      "[Epoch 10] Training Accuracy: 57.86140724946695\n",
      "[Epoch 10] Test Accuracy: 55.98089171974522\n",
      "[Epoch 10] Test Loss: 0.36881697737866903\n",
      "############################################################\n",
      "[Epoch 11] Training Loss = 0.24099929781273158 (6336/60000)\n",
      "[Epoch 11] Training Loss = 0.2402118678443396 (12736/60000)\n",
      "[Epoch 11] Training Loss = 0.2421649727833311 (19136/60000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Training Loss = 0.24408170276491864 (25536/60000)\n",
      "[Epoch 11] Training Loss = 0.2449200382094106 (31936/60000)\n",
      "[Epoch 11] Training Loss = 0.24528015851476953 (38336/60000)\n",
      "[Epoch 11] Training Loss = 0.24840551890827214 (44736/60000)\n",
      "[Epoch 11] Training Loss = 0.25011713340300346 (51136/60000)\n",
      "[Epoch 11] Training Loss = 0.25117359175300175 (57536/60000)\n",
      "[Epoch 11] Training Accuracy: 57.97867803837953\n",
      "[Epoch 11] Test Accuracy: 55.97452229299363\n",
      "[Epoch 11] Test Loss: 0.37198831938254606\n",
      "############################################################\n",
      "[Epoch 12] Training Loss = 0.23489907888149975 (6336/60000)\n",
      "[Epoch 12] Training Loss = 0.23665843211376486 (12736/60000)\n",
      "[Epoch 12] Training Loss = 0.2395379848803166 (19136/60000)\n",
      "[Epoch 12] Training Loss = 0.24477135748567438 (25536/60000)\n",
      "[Epoch 12] Training Loss = 0.24815147579134347 (31936/60000)\n",
      "[Epoch 12] Training Loss = 0.24604480533871506 (38336/60000)\n",
      "[Epoch 12] Training Loss = 0.24692090459498384 (44736/60000)\n",
      "[Epoch 12] Training Loss = 0.24704722243737667 (51136/60000)\n",
      "[Epoch 12] Training Loss = 0.24725593938528498 (57536/60000)\n",
      "[Epoch 12] Training Accuracy: 58.10234541577825\n",
      "[Epoch 12] Test Accuracy: 55.904458598726116\n",
      "[Epoch 12] Test Loss: 0.374445310823477\n",
      "############################################################\n",
      "[Epoch 13] Training Loss = 0.24457830457825852 (6336/60000)\n",
      "[Epoch 13] Training Loss = 0.24003662784869348 (12736/60000)\n",
      "[Epoch 13] Training Loss = 0.24178909177714367 (19136/60000)\n",
      "[Epoch 13] Training Loss = 0.24234818539262415 (25536/60000)\n",
      "[Epoch 13] Training Loss = 0.2419925520424255 (31936/60000)\n",
      "[Epoch 13] Training Loss = 0.2416033159027215 (38336/60000)\n",
      "[Epoch 13] Training Loss = 0.24119556360000022 (44736/60000)\n",
      "[Epoch 13] Training Loss = 0.24060346176369915 (51136/60000)\n",
      "[Epoch 13] Training Loss = 0.23921949752048072 (57536/60000)\n",
      "[Epoch 13] Training Accuracy: 58.343283582089555\n",
      "[Epoch 13] Test Accuracy: 55.66242038216561\n",
      "[Epoch 13] Test Loss: 0.36859014146267227\n",
      "############################################################\n",
      "[Epoch 14] Training Loss = 0.2292123193843196 (6336/60000)\n",
      "[Epoch 14] Training Loss = 0.2245662692353953 (12736/60000)\n",
      "[Epoch 14] Training Loss = 0.22610810443908474 (19136/60000)\n",
      "[Epoch 14] Training Loss = 0.22592236699167648 (25536/60000)\n",
      "[Epoch 14] Training Loss = 0.22881678178279338 (31936/60000)\n",
      "[Epoch 14] Training Loss = 0.23010263713900753 (38336/60000)\n",
      "[Epoch 14] Training Loss = 0.23151634724141873 (44736/60000)\n",
      "[Epoch 14] Training Loss = 0.23143129159832776 (51136/60000)\n",
      "[Epoch 14] Training Loss = 0.23013392376720707 (57536/60000)\n",
      "[Epoch 14] Training Accuracy: 58.495735607675904\n",
      "[Epoch 14] Test Accuracy: 56.46496815286624\n",
      "[Epoch 14] Test Loss: 0.37315610174540503\n",
      "############################################################\n",
      "[Epoch 15] Training Loss = 0.2563295548010354 (6336/60000)\n",
      "[Epoch 15] Training Loss = 0.23332503924717257 (12736/60000)\n",
      "[Epoch 15] Training Loss = 0.2381458352740393 (19136/60000)\n",
      "[Epoch 15] Training Loss = 0.2352495529411132 (25536/60000)\n",
      "[Epoch 15] Training Loss = 0.23350311203566726 (31936/60000)\n",
      "[Epoch 15] Training Loss = 0.2345121413444438 (38336/60000)\n",
      "[Epoch 15] Training Loss = 0.2358733110545531 (44736/60000)\n",
      "[Epoch 15] Training Loss = 0.23688227810124132 (51136/60000)\n",
      "[Epoch 15] Training Loss = 0.23524566809413697 (57536/60000)\n",
      "[Epoch 15] Training Accuracy: 58.360341151385924\n",
      "[Epoch 15] Test Accuracy: 56.089171974522294\n",
      "[Epoch 15] Test Loss: 0.3733416625838371\n",
      "############################################################\n",
      "[Epoch 16] Training Loss = 0.2334539428579085 (6336/60000)\n",
      "[Epoch 16] Training Loss = 0.23005893835844707 (12736/60000)\n",
      "[Epoch 16] Training Loss = 0.2314192575580101 (19136/60000)\n",
      "[Epoch 16] Training Loss = 0.22441262401696435 (25536/60000)\n",
      "[Epoch 16] Training Loss = 0.22763114194861633 (31936/60000)\n",
      "[Epoch 16] Training Loss = 0.22539739515352528 (38336/60000)\n",
      "[Epoch 16] Training Loss = 0.2246100680159141 (44736/60000)\n",
      "[Epoch 16] Training Loss = 0.2259183797346561 (51136/60000)\n",
      "[Epoch 16] Training Loss = 0.22638081408647595 (57536/60000)\n",
      "[Epoch 16] Training Accuracy: 58.58102345415778\n",
      "[Epoch 16] Test Accuracy: 55.97452229299363\n",
      "[Epoch 16] Test Loss: 0.3865028006161094\n",
      "############################################################\n",
      "[Epoch 17] Training Loss = 0.21715150111251408 (6336/60000)\n",
      "[Epoch 17] Training Loss = 0.22314112440650188 (12736/60000)\n",
      "[Epoch 17] Training Loss = 0.2243712289402118 (19136/60000)\n",
      "[Epoch 17] Training Loss = 0.22236842046816246 (25536/60000)\n",
      "[Epoch 17] Training Loss = 0.2207943495579497 (31936/60000)\n",
      "[Epoch 17] Training Loss = 0.2201235075862161 (38336/60000)\n",
      "[Epoch 17] Training Loss = 0.221201688205778 (44736/60000)\n",
      "[Epoch 17] Training Loss = 0.22016885104089118 (51136/60000)\n",
      "[Epoch 17] Training Loss = 0.22014039198353108 (57536/60000)\n",
      "[Epoch 17] Training Accuracy: 58.72601279317697\n",
      "[Epoch 17] Test Accuracy: 56.0828025477707\n",
      "[Epoch 17] Test Loss: 0.3924122559037178\n",
      "############################################################\n",
      "[Epoch 18] Training Loss = 0.20455851895038527 (6336/60000)\n",
      "[Epoch 18] Training Loss = 0.2117245544740303 (12736/60000)\n",
      "[Epoch 18] Training Loss = 0.21619672510137525 (19136/60000)\n",
      "[Epoch 18] Training Loss = 0.21419572767645195 (25536/60000)\n",
      "[Epoch 18] Training Loss = 0.2135879224565679 (31936/60000)\n",
      "[Epoch 18] Training Loss = 0.21139804794389339 (38336/60000)\n",
      "[Epoch 18] Training Loss = 0.21224272076855913 (44736/60000)\n",
      "[Epoch 18] Training Loss = 0.21163520549840115 (51136/60000)\n",
      "[Epoch 18] Training Loss = 0.2120302998035052 (57536/60000)\n",
      "[Epoch 18] Training Accuracy: 58.87953091684435\n",
      "[Epoch 18] Test Accuracy: 56.12738853503185\n",
      "[Epoch 18] Test Loss: 0.39429921374484234\n",
      "############################################################\n",
      "[Epoch 19] Training Loss = 0.20584659157979368 (6336/60000)\n",
      "[Epoch 19] Training Loss = 0.21630306085150444 (12736/60000)\n",
      "[Epoch 19] Training Loss = 0.22007946313723273 (19136/60000)\n",
      "[Epoch 19] Training Loss = 0.21779050290547217 (25536/60000)\n",
      "[Epoch 19] Training Loss = 0.2173940816106562 (31936/60000)\n",
      "[Epoch 19] Training Loss = 0.21540256526837365 (38336/60000)\n",
      "[Epoch 19] Training Loss = 0.2175567286283588 (44736/60000)\n",
      "[Epoch 19] Training Loss = 0.21740435329131846 (51136/60000)\n",
      "[Epoch 19] Training Loss = 0.21643108011402595 (57536/60000)\n",
      "[Epoch 19] Training Accuracy: 58.89232409381663\n",
      "[Epoch 19] Test Accuracy: 55.82165605095541\n",
      "[Epoch 19] Test Loss: 0.41113567969221976\n",
      "############################################################\n",
      "[Epoch 20] Training Loss = 0.2070993153434811 (6336/60000)\n",
      "[Epoch 20] Training Loss = 0.20525703414450938 (12736/60000)\n",
      "[Epoch 20] Training Loss = 0.199581047825291 (19136/60000)\n",
      "[Epoch 20] Training Loss = 0.19955901011712568 (25536/60000)\n",
      "[Epoch 20] Training Loss = 0.20135162473620777 (31936/60000)\n",
      "[Epoch 20] Training Loss = 0.20403347280069264 (38336/60000)\n",
      "[Epoch 20] Training Loss = 0.2036822014814709 (44736/60000)\n",
      "[Epoch 20] Training Loss = 0.20492690962474694 (51136/60000)\n",
      "[Epoch 20] Training Loss = 0.2070666541106112 (57536/60000)\n",
      "[Epoch 20] Training Accuracy: 59.03837953091684\n",
      "[Epoch 20] Test Accuracy: 55.57324840764331\n",
      "[Epoch 20] Test Loss: 0.40802364340823166\n",
      "############################################################\n",
      "[Epoch 21] Training Loss = 0.21110428572453635 (6336/60000)\n",
      "[Epoch 21] Training Loss = 0.21548033782734943 (12736/60000)\n",
      "[Epoch 21] Training Loss = 0.21249135279635523 (19136/60000)\n",
      "[Epoch 21] Training Loss = 0.20857611690696917 (25536/60000)\n",
      "[Epoch 21] Training Loss = 0.20574268129820336 (31936/60000)\n",
      "[Epoch 21] Training Loss = 0.20403516705525737 (38336/60000)\n",
      "[Epoch 21] Training Loss = 0.20355327431650802 (44736/60000)\n",
      "[Epoch 21] Training Loss = 0.2034663571946985 (51136/60000)\n",
      "[Epoch 21] Training Loss = 0.2052384228367893 (57536/60000)\n",
      "[Epoch 21] Training Accuracy: 59.11194029850746\n",
      "[Epoch 21] Test Accuracy: 56.095541401273884\n",
      "[Epoch 21] Test Loss: 0.41247629597308527\n",
      "############################################################\n",
      "[Epoch 22] Training Loss = 0.1960570438567436 (6336/60000)\n",
      "[Epoch 22] Training Loss = 0.1994309992197171 (12736/60000)\n",
      "[Epoch 22] Training Loss = 0.1945138176822144 (19136/60000)\n",
      "[Epoch 22] Training Loss = 0.19873810096417455 (25536/60000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Training Loss = 0.19729410026050762 (31936/60000)\n",
      "[Epoch 22] Training Loss = 0.20076142408836864 (38336/60000)\n",
      "[Epoch 22] Training Loss = 0.19985317875856323 (44736/60000)\n",
      "[Epoch 22] Training Loss = 0.20090919740973634 (51136/60000)\n",
      "[Epoch 22] Training Loss = 0.2053740549961041 (57536/60000)\n",
      "[Epoch 22] Training Accuracy: 59.11513859275053\n",
      "[Epoch 22] Test Accuracy: 56.10191082802548\n",
      "[Epoch 22] Test Loss: 0.3954176713896405\n",
      "############################################################\n",
      "[Epoch 23] Training Loss = 0.20171638514206866 (6336/60000)\n",
      "[Epoch 23] Training Loss = 0.1948249153044056 (12736/60000)\n",
      "[Epoch 23] Training Loss = 0.1960644316314455 (19136/60000)\n",
      "[Epoch 23] Training Loss = 0.1993336396240501 (25536/60000)\n",
      "[Epoch 23] Training Loss = 0.19647390850799118 (31936/60000)\n",
      "[Epoch 23] Training Loss = 0.19799070721974157 (38336/60000)\n",
      "[Epoch 23] Training Loss = 0.19747840940888348 (44736/60000)\n",
      "[Epoch 23] Training Loss = 0.19781865332933032 (51136/60000)\n",
      "[Epoch 23] Training Loss = 0.19612509465555725 (57536/60000)\n",
      "[Epoch 23] Training Accuracy: 59.24626865671642\n",
      "[Epoch 23] Test Accuracy: 55.62420382165605\n",
      "[Epoch 23] Test Loss: 0.452483202384156\n",
      "############################################################\n",
      "[Epoch 24] Training Loss = 0.18771814746838628 (6336/60000)\n",
      "[Epoch 24] Training Loss = 0.19262283263568902 (12736/60000)\n",
      "[Epoch 24] Training Loss = 0.1930155761315671 (19136/60000)\n",
      "[Epoch 24] Training Loss = 0.19313941544906837 (25536/60000)\n",
      "[Epoch 24] Training Loss = 0.1928230709655729 (31936/60000)\n",
      "[Epoch 24] Training Loss = 0.19034454394411762 (38336/60000)\n",
      "[Epoch 24] Training Loss = 0.19360957405022286 (44736/60000)\n",
      "[Epoch 24] Training Loss = 0.194050683321732 (51136/60000)\n",
      "[Epoch 24] Training Loss = 0.19528134755616194 (57536/60000)\n",
      "[Epoch 24] Training Accuracy: 59.362473347547976\n",
      "[Epoch 24] Test Accuracy: 56.01273885350319\n",
      "[Epoch 24] Test Loss: 0.42544153845234284\n",
      "############################################################\n",
      "[Epoch 25] Training Loss = 0.1911008447935485 (6336/60000)\n",
      "[Epoch 25] Training Loss = 0.18871007384217564 (12736/60000)\n",
      "[Epoch 25] Training Loss = 0.1856893016069031 (19136/60000)\n",
      "[Epoch 25] Training Loss = 0.1861741186252662 (25536/60000)\n",
      "[Epoch 25] Training Loss = 0.18786317817045595 (31936/60000)\n",
      "[Epoch 25] Training Loss = 0.19221075949490768 (38336/60000)\n",
      "[Epoch 25] Training Loss = 0.19262774317966339 (44736/60000)\n",
      "[Epoch 25] Training Loss = 0.19483967359470336 (51136/60000)\n",
      "[Epoch 25] Training Loss = 0.19420289380001413 (57536/60000)\n",
      "[Epoch 25] Training Accuracy: 59.37953091684435\n",
      "[Epoch 25] Test Accuracy: 56.07006369426752\n",
      "[Epoch 25] Test Loss: 0.42171230494596396\n",
      "############################################################\n",
      "[Epoch 26] Training Loss = 0.18956781611448587 (6336/60000)\n",
      "[Epoch 26] Training Loss = 0.19002823821788456 (12736/60000)\n",
      "[Epoch 26] Training Loss = 0.18160371592012936 (19136/60000)\n",
      "[Epoch 26] Training Loss = 0.1766548741954311 (25536/60000)\n",
      "[Epoch 26] Training Loss = 0.17936362148465995 (31936/60000)\n",
      "[Epoch 26] Training Loss = 0.18197885969768782 (38336/60000)\n",
      "[Epoch 26] Training Loss = 0.1838751113220772 (44736/60000)\n",
      "[Epoch 26] Training Loss = 0.18563574396810484 (51136/60000)\n",
      "[Epoch 26] Training Loss = 0.18819675969997954 (57536/60000)\n",
      "[Epoch 26] Training Accuracy: 59.46481876332623\n",
      "[Epoch 26] Test Accuracy: 56.50955414012739\n",
      "[Epoch 26] Test Loss: 0.40489129565513815\n",
      "############################################################\n",
      "[Epoch 27] Training Loss = 0.17699486818729024 (6336/60000)\n",
      "[Epoch 27] Training Loss = 0.1784255005344374 (12736/60000)\n",
      "[Epoch 27] Training Loss = 0.1917028572024111 (19136/60000)\n",
      "[Epoch 27] Training Loss = 0.19183373689315372 (25536/60000)\n",
      "[Epoch 27] Training Loss = 0.19266065028692056 (31936/60000)\n",
      "[Epoch 27] Training Loss = 0.19036642501196002 (38336/60000)\n",
      "[Epoch 27] Training Loss = 0.190777420395391 (44736/60000)\n",
      "[Epoch 27] Training Loss = 0.19347468545387625 (51136/60000)\n",
      "[Epoch 27] Training Loss = 0.19265125919386197 (57536/60000)\n",
      "[Epoch 27] Training Accuracy: 59.46268656716418\n",
      "[Epoch 27] Test Accuracy: 55.98089171974522\n",
      "[Epoch 27] Test Loss: 0.43906100689890276\n",
      "############################################################\n",
      "[Epoch 28] Training Loss = 0.17652608559589195 (6336/60000)\n",
      "[Epoch 28] Training Loss = 0.18206201383291776 (12736/60000)\n",
      "[Epoch 28] Training Loss = 0.1888767417682254 (19136/60000)\n",
      "[Epoch 28] Training Loss = 0.1873818829487589 (25536/60000)\n",
      "[Epoch 28] Training Loss = 0.18810584486456577 (31936/60000)\n",
      "[Epoch 28] Training Loss = 0.18700412106086098 (38336/60000)\n",
      "[Epoch 28] Training Loss = 0.18503337421684987 (44736/60000)\n",
      "[Epoch 28] Training Loss = 0.1831123438268639 (51136/60000)\n",
      "[Epoch 28] Training Loss = 0.18289428817997122 (57536/60000)\n",
      "[Epoch 28] Training Accuracy: 59.58742004264392\n",
      "[Epoch 28] Test Accuracy: 56.203821656050955\n",
      "[Epoch 28] Test Loss: 0.43000108819858285\n",
      "############################################################\n",
      "[Epoch 29] Training Loss = 0.17337149118233208 (6336/60000)\n",
      "[Epoch 29] Training Loss = 0.1680280312184413 (12736/60000)\n",
      "[Epoch 29] Training Loss = 0.16662624092106038 (19136/60000)\n",
      "[Epoch 29] Training Loss = 0.1740569921028345 (25536/60000)\n",
      "[Epoch 29] Training Loss = 0.17720120545021278 (31936/60000)\n",
      "[Epoch 29] Training Loss = 0.17809155390586998 (38336/60000)\n",
      "[Epoch 29] Training Loss = 0.17818791858414895 (44736/60000)\n",
      "[Epoch 29] Training Loss = 0.17839404286464702 (51136/60000)\n",
      "[Epoch 29] Training Loss = 0.17825072606791378 (57536/60000)\n",
      "[Epoch 29] Training Accuracy: 59.75799573560768\n",
      "[Epoch 29] Test Accuracy: 56.261146496815286\n",
      "[Epoch 29] Test Loss: 0.455335918078377\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    print(\"#\"*60)\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = torch.sum(top_class == labels.to(model.device).view(*top_class.shape))\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor)) \n",
    "        loss = criterion(log_ps, labels.to(model.device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"[Epoch {0}] Training Loss = {1} ({2}/{3})\".format(e, running_loss / i, 64 * i, len(trainset)))\n",
    "#             print('[Epoch {0}] Training Accuracy: {1}'.format(e, accuracy.item() / i))\n",
    "    \n",
    "    print('[Epoch {0}] Training Accuracy: {1}'.format(e, accuracy.item() / len(trainloader)))\n",
    "    \n",
    "    ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        test_loss = 0\n",
    "        for i, (images, labels) in enumerate(testloader):\n",
    "            log_ps = model(images)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = torch.sum(top_class == labels.to(model.device).view(*top_class.shape))\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)) \n",
    "            loss = criterion(log_ps, labels.to(model.device))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        print('[Epoch {0}] Test Accuracy: {1}'.format(e, accuracy.item() / len(testloader)))\n",
    "        print('[Epoch {0}] Test Loss: {1}'.format(e, test_loss / len(testloader)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.\n",
    "\n",
    "<img src='assets/overfitting.png' width=450px>\n",
    "\n",
    "The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called *early-stopping*. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss.\n",
    "\n",
    "The most common method to reduce overfitting (outside of early-stopping) is *dropout*, where we randomly drop input units. This forces the network to share information between weights, increasing it's ability to generalize to new data. Adding dropout in PyTorch is straightforward using the [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) module.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Add dropout to your model and train it on Fashion-MNIST again. See if you can get a lower validation loss or higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define your model with dropout added\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        if self.use_gpu:\n",
    "            self.device = torch.device('cuda:0')\n",
    "            self.cuda(device=self.device)\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        if self.use_gpu:\n",
    "            x = x.to(self.device)\n",
    "            \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, network, learning_rate=0.003):\n",
    "        self.network = network\n",
    "        self.test_loss_hist = []\n",
    "        self.train_loss_hist = []\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "#         self.optimizer = optim.LBFGS(self.network.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def partial_fit(self, e, verbose=False):\n",
    "        running_loss = 0\n",
    "        for i, (images, labels) in enumerate(self.trainloader):\n",
    "            self.optimizer.zero_grad()\n",
    "            log_ps = self.network(images)\n",
    "            loss = self.criterion(log_ps, labels.to(self.network.device))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 100 == 0 and verbose:\n",
    "                print(\"[Epoch {0}] Training Loss = {1} ({2}/{3})\".format(e, \n",
    "                                                                         running_loss / i, \n",
    "                                                                         64 * i, \n",
    "                                                                         len(self.trainloader) * 64))\n",
    "        self.train_loss_hist.append(running_loss / len(self.trainloader))\n",
    "        \n",
    "        \n",
    "        if self.testloader is not None:\n",
    "            with torch.no_grad():\n",
    "                running_loss = 0\n",
    "                for i, (images, labels) in enumerate(self.testloader):\n",
    "                    log_ps = self.network(images)\n",
    "                    loss = self.criterion(log_ps, labels.to(self.network.device))\n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                if verbose:\n",
    "                    print(\"[Epoch {0}] Test Loss = {1}\".format(e, running_loss / len(self.testloader)))\n",
    "                self.test_loss_hist.append(running_loss / len(self.testloader))\n",
    "                      \n",
    "    def fit(self, trainloader, testloader=None, epochs=10, verbose=False):\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.network.train()\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            self.partial_fit(e, verbose=verbose)\n",
    "    \n",
    "    def evaluate(self, evalloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0\n",
    "            accuracy = 0\n",
    "            for i, (images, labels) in enumerate(evalloader):\n",
    "                log_ps = self.network(images)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.to(self.network.device).view(*top_class.shape)\n",
    "                loss = self.criterion(log_ps, labels.to(self.network.device))\n",
    "                running_loss += loss.item()\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        print(\"Loss: {0}\".format(running_loss / len(evalloader)))\n",
    "        print(\"Accuracy: {0}\".format(accuracy / len(evalloader)))\n",
    "                \n",
    "    def predict(self, image):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            log_ps = self.network.forward(image)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "        \n",
    "        return ps, top_class\n",
    "    \n",
    "    def plot_hist(self):\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        plt.plot(self.train_loss_hist, '-b', lw=2, label=\"Train Loss\")\n",
    "        if self.testloader is not None:\n",
    "            plt.plot(self.test_loss_hist, '-r', lw=2, label=\"Test Loss\")\n",
    "        plt.legend(prop={'size' : 18})\n",
    "        plt.title(\"Training vs. Test Loss\", size=18)\n",
    "        plt.xlabel(\"Epochs\", size=18)\n",
    "        plt.ylabel(\"Loss\", size=18)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Training Loss = 1.5451089047422313 (6336/60032)\n",
      "[Epoch 0] Training Loss = 1.4173203940367578 (12736/60032)\n",
      "[Epoch 0] Training Loss = 1.3236924565356711 (19136/60032)\n",
      "[Epoch 0] Training Loss = 1.2678780706604023 (25536/60032)\n",
      "[Epoch 0] Training Loss = 1.2272463083745004 (31936/60032)\n",
      "[Epoch 0] Training Loss = 1.1942842628601595 (38336/60032)\n",
      "[Epoch 0] Training Loss = 1.1710072638991906 (44736/60032)\n",
      "[Epoch 0] Training Loss = 1.1497993559503137 (51136/60032)\n",
      "[Epoch 0] Training Loss = 1.1302027365522205 (57536/60032)\n",
      "[Epoch 0] Test Loss = 1.0249515426386693\n",
      "[Epoch 1] Training Loss = 1.0085882869633762 (6336/60032)\n",
      "[Epoch 1] Training Loss = 1.004673534601777 (12736/60032)\n",
      "[Epoch 1] Training Loss = 0.9931526664507428 (19136/60032)\n",
      "[Epoch 1] Training Loss = 0.9834630130825186 (25536/60032)\n",
      "[Epoch 1] Training Loss = 0.9815596253695135 (31936/60032)\n",
      "[Epoch 1] Training Loss = 0.9707083248335849 (38336/60032)\n",
      "[Epoch 1] Training Loss = 0.9678802641590948 (44736/60032)\n",
      "[Epoch 1] Training Loss = 0.9678240342716102 (51136/60032)\n",
      "[Epoch 1] Training Loss = 0.9672429645485819 (57536/60032)\n",
      "[Epoch 1] Test Loss = 0.979953000499944\n",
      "[Epoch 2] Training Loss = 0.9500659132244611 (6336/60032)\n",
      "[Epoch 2] Training Loss = 0.948374710789877 (12736/60032)\n",
      "[Epoch 2] Training Loss = 0.9364666007832939 (19136/60032)\n",
      "[Epoch 2] Training Loss = 0.9354659664003473 (25536/60032)\n",
      "[Epoch 2] Training Loss = 0.9408835130846334 (31936/60032)\n",
      "[Epoch 2] Training Loss = 0.9382115680108684 (38336/60032)\n",
      "[Epoch 2] Training Loss = 0.9384963367459429 (44736/60032)\n",
      "[Epoch 2] Training Loss = 0.9345163129298052 (51136/60032)\n",
      "[Epoch 2] Training Loss = 0.9294717718284573 (57536/60032)\n",
      "[Epoch 2] Test Loss = 0.9409474347047745\n",
      "[Epoch 3] Training Loss = 0.9194536853318263 (6336/60032)\n",
      "[Epoch 3] Training Loss = 0.9070184197857152 (12736/60032)\n",
      "[Epoch 3] Training Loss = 0.9125193389363113 (19136/60032)\n",
      "[Epoch 3] Training Loss = 0.9120371529930517 (25536/60032)\n",
      "[Epoch 3] Training Loss = 0.9130688259501256 (31936/60032)\n",
      "[Epoch 3] Training Loss = 0.9119084633152951 (38336/60032)\n",
      "[Epoch 3] Training Loss = 0.9085794500100596 (44736/60032)\n",
      "[Epoch 3] Training Loss = 0.9051975682098069 (51136/60032)\n",
      "[Epoch 3] Training Loss = 0.9048238578640977 (57536/60032)\n",
      "[Epoch 3] Test Loss = 0.9663664928287458\n",
      "[Epoch 4] Training Loss = 0.8951419805637514 (6336/60032)\n",
      "[Epoch 4] Training Loss = 0.8936263222490722 (12736/60032)\n",
      "[Epoch 4] Training Loss = 0.8985607830377725 (19136/60032)\n",
      "[Epoch 4] Training Loss = 0.9076188670513325 (25536/60032)\n",
      "[Epoch 4] Training Loss = 0.9020984759908879 (31936/60032)\n",
      "[Epoch 4] Training Loss = 0.8989953585280003 (38336/60032)\n",
      "[Epoch 4] Training Loss = 0.899442951396129 (44736/60032)\n",
      "[Epoch 4] Training Loss = 0.8998124470400423 (51136/60032)\n",
      "[Epoch 4] Training Loss = 0.89605507032492 (57536/60032)\n",
      "[Epoch 4] Test Loss = 0.9504620561933821\n",
      "[Epoch 5] Training Loss = 0.9003231790330675 (6336/60032)\n",
      "[Epoch 5] Training Loss = 0.8934519018360119 (12736/60032)\n",
      "[Epoch 5] Training Loss = 0.8923336145869865 (19136/60032)\n",
      "[Epoch 5] Training Loss = 0.8967460419301102 (25536/60032)\n",
      "[Epoch 5] Training Loss = 0.8863184454445848 (31936/60032)\n",
      "[Epoch 5] Training Loss = 0.8863193546989326 (38336/60032)\n",
      "[Epoch 5] Training Loss = 0.8853907619252567 (44736/60032)\n",
      "[Epoch 5] Training Loss = 0.8844990431143435 (51136/60032)\n",
      "[Epoch 5] Training Loss = 0.8847303180991609 (57536/60032)\n",
      "[Epoch 5] Test Loss = 0.945763220832606\n",
      "[Epoch 6] Training Loss = 0.8651173042528557 (6336/60032)\n",
      "[Epoch 6] Training Loss = 0.8648580133016385 (12736/60032)\n",
      "[Epoch 6] Training Loss = 0.8770619499244817 (19136/60032)\n",
      "[Epoch 6] Training Loss = 0.8839380399027564 (25536/60032)\n",
      "[Epoch 6] Training Loss = 0.884517403487452 (31936/60032)\n",
      "[Epoch 6] Training Loss = 0.8833805134081483 (38336/60032)\n",
      "[Epoch 6] Training Loss = 0.8797239543115973 (44736/60032)\n",
      "[Epoch 6] Training Loss = 0.8797124858493351 (51136/60032)\n",
      "[Epoch 6] Training Loss = 0.8804620470565736 (57536/60032)\n",
      "[Epoch 6] Test Loss = 0.9288021159020199\n",
      "[Epoch 7] Training Loss = 0.8926412065823873 (6336/60032)\n",
      "[Epoch 7] Training Loss = 0.870459037210474 (12736/60032)\n",
      "[Epoch 7] Training Loss = 0.8648789445293388 (19136/60032)\n",
      "[Epoch 7] Training Loss = 0.8644793574373824 (25536/60032)\n",
      "[Epoch 7] Training Loss = 0.8612431528453598 (31936/60032)\n",
      "[Epoch 7] Training Loss = 0.8634016425760839 (38336/60032)\n",
      "[Epoch 7] Training Loss = 0.8675169421623705 (44736/60032)\n",
      "[Epoch 7] Training Loss = 0.8707275755787374 (51136/60032)\n",
      "[Epoch 7] Training Loss = 0.8706927709571511 (57536/60032)\n",
      "[Epoch 7] Test Loss = 0.8997176281965462\n",
      "[Epoch 8] Training Loss = 0.879018239601694 (6336/60032)\n",
      "[Epoch 8] Training Loss = 0.8557425267133282 (12736/60032)\n",
      "[Epoch 8] Training Loss = 0.8720532773330458 (19136/60032)\n",
      "[Epoch 8] Training Loss = 0.864792479608292 (25536/60032)\n",
      "[Epoch 8] Training Loss = 0.8659832569544683 (31936/60032)\n",
      "[Epoch 8] Training Loss = 0.8693593677773898 (38336/60032)\n",
      "[Epoch 8] Training Loss = 0.86942570028728 (44736/60032)\n",
      "[Epoch 8] Training Loss = 0.8681137581566846 (51136/60032)\n",
      "[Epoch 8] Training Loss = 0.8682264926303613 (57536/60032)\n",
      "[Epoch 8] Test Loss = 0.9505841500440221\n",
      "[Epoch 9] Training Loss = 0.8655793727046311 (6336/60032)\n",
      "[Epoch 9] Training Loss = 0.8514257748821872 (12736/60032)\n",
      "[Epoch 9] Training Loss = 0.8597436355906586 (19136/60032)\n",
      "[Epoch 9] Training Loss = 0.8518332238484146 (25536/60032)\n",
      "[Epoch 9] Training Loss = 0.8627082345958702 (31936/60032)\n",
      "[Epoch 9] Training Loss = 0.8630341322772292 (38336/60032)\n",
      "[Epoch 9] Training Loss = 0.8626540359082993 (44736/60032)\n",
      "[Epoch 9] Training Loss = 0.8616974262406442 (51136/60032)\n",
      "[Epoch 9] Training Loss = 0.8622902505456141 (57536/60032)\n",
      "[Epoch 9] Test Loss = 0.9291623018349812\n",
      "[Epoch 10] Training Loss = 0.8946797010874508 (6336/60032)\n",
      "[Epoch 10] Training Loss = 0.8649413367012637 (12736/60032)\n",
      "[Epoch 10] Training Loss = 0.8686315110295911 (19136/60032)\n",
      "[Epoch 10] Training Loss = 0.8671317792177798 (25536/60032)\n",
      "[Epoch 10] Training Loss = 0.8611305221049246 (31936/60032)\n",
      "[Epoch 10] Training Loss = 0.8617095432814851 (38336/60032)\n",
      "[Epoch 10] Training Loss = 0.863563612380594 (44736/60032)\n",
      "[Epoch 10] Training Loss = 0.863023666177733 (51136/60032)\n",
      "[Epoch 10] Training Loss = 0.8595386841339583 (57536/60032)\n",
      "[Epoch 10] Test Loss = 0.8980031028674667\n",
      "[Epoch 11] Training Loss = 0.8430260630569073 (6336/60032)\n",
      "[Epoch 11] Training Loss = 0.8473826869947827 (12736/60032)\n",
      "[Epoch 11] Training Loss = 0.8508395138990918 (19136/60032)\n",
      "[Epoch 11] Training Loss = 0.8579561085181129 (25536/60032)\n",
      "[Epoch 11] Training Loss = 0.8574092581061896 (31936/60032)\n",
      "[Epoch 11] Training Loss = 0.8566783864131953 (38336/60032)\n",
      "[Epoch 11] Training Loss = 0.8542808398753617 (44736/60032)\n",
      "[Epoch 11] Training Loss = 0.8541491272228681 (51136/60032)\n",
      "[Epoch 11] Training Loss = 0.8524560981717073 (57536/60032)\n",
      "[Epoch 11] Test Loss = 0.9458959930261989\n",
      "[Epoch 12] Training Loss = 0.8432402794409279 (6336/60032)\n",
      "[Epoch 12] Training Loss = 0.8524061056537244 (12736/60032)\n",
      "[Epoch 12] Training Loss = 0.8481802347312404 (19136/60032)\n",
      "[Epoch 12] Training Loss = 0.8446163072771297 (25536/60032)\n",
      "[Epoch 12] Training Loss = 0.8506375071639288 (31936/60032)\n",
      "[Epoch 12] Training Loss = 0.8535295449732142 (38336/60032)\n",
      "[Epoch 12] Training Loss = 0.855022610981918 (44736/60032)\n",
      "[Epoch 12] Training Loss = 0.8552399344080232 (51136/60032)\n",
      "[Epoch 12] Training Loss = 0.8541517338577712 (57536/60032)\n",
      "[Epoch 12] Test Loss = 0.9174093152307401\n",
      "[Epoch 13] Training Loss = 0.8557803203361203 (6336/60032)\n",
      "[Epoch 13] Training Loss = 0.8424837454479543 (12736/60032)\n",
      "[Epoch 13] Training Loss = 0.8492070115529574 (19136/60032)\n",
      "[Epoch 13] Training Loss = 0.8477498809795332 (25536/60032)\n",
      "[Epoch 13] Training Loss = 0.8445453954364112 (31936/60032)\n",
      "[Epoch 13] Training Loss = 0.847377212795869 (38336/60032)\n",
      "[Epoch 13] Training Loss = 0.8547449981705825 (44736/60032)\n",
      "[Epoch 13] Training Loss = 0.8511210804588356 (51136/60032)\n",
      "[Epoch 13] Training Loss = 0.8515187903028707 (57536/60032)\n",
      "[Epoch 13] Test Loss = 0.8911380612166824\n",
      "[Epoch 14] Training Loss = 0.8697719128444942 (6336/60032)\n",
      "[Epoch 14] Training Loss = 0.8473690379804103 (12736/60032)\n",
      "[Epoch 14] Training Loss = 0.8530429893712136 (19136/60032)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Training Loss = 0.8471017287936724 (25536/60032)\n",
      "[Epoch 14] Training Loss = 0.8455298481460564 (31936/60032)\n",
      "[Epoch 14] Training Loss = 0.845765371453981 (38336/60032)\n",
      "[Epoch 14] Training Loss = 0.8471798871208158 (44736/60032)\n",
      "[Epoch 14] Training Loss = 0.8454526742647527 (51136/60032)\n",
      "[Epoch 14] Training Loss = 0.8473734965048589 (57536/60032)\n",
      "[Epoch 14] Test Loss = 0.9093506188149665\n",
      "[Epoch 15] Training Loss = 0.8426842271077513 (6336/60032)\n",
      "[Epoch 15] Training Loss = 0.8396792224603682 (12736/60032)\n",
      "[Epoch 15] Training Loss = 0.8400731213316072 (19136/60032)\n",
      "[Epoch 15] Training Loss = 0.8438795429273954 (25536/60032)\n",
      "[Epoch 15] Training Loss = 0.8471964106172741 (31936/60032)\n",
      "[Epoch 15] Training Loss = 0.8477123213033246 (38336/60032)\n",
      "[Epoch 15] Training Loss = 0.8489037655368554 (44736/60032)\n",
      "[Epoch 15] Training Loss = 0.8496749136265885 (51136/60032)\n",
      "[Epoch 15] Training Loss = 0.8527446881815112 (57536/60032)\n",
      "[Epoch 15] Test Loss = 0.9350180652490847\n",
      "[Epoch 16] Training Loss = 0.855660348829597 (6336/60032)\n",
      "[Epoch 16] Training Loss = 0.8402570995254133 (12736/60032)\n",
      "[Epoch 16] Training Loss = 0.8475202322006226 (19136/60032)\n",
      "[Epoch 16] Training Loss = 0.8467542715837484 (25536/60032)\n",
      "[Epoch 16] Training Loss = 0.8451015332299388 (31936/60032)\n",
      "[Epoch 16] Training Loss = 0.8428125320272971 (38336/60032)\n",
      "[Epoch 16] Training Loss = 0.8467183675462425 (44736/60032)\n",
      "[Epoch 16] Training Loss = 0.8485652409372699 (51136/60032)\n",
      "[Epoch 16] Training Loss = 0.8505656177528178 (57536/60032)\n",
      "[Epoch 16] Test Loss = 0.9160054390597495\n",
      "[Epoch 17] Training Loss = 0.8345427461946854 (6336/60032)\n",
      "[Epoch 17] Training Loss = 0.8279845892783985 (12736/60032)\n",
      "[Epoch 17] Training Loss = 0.8293651856706294 (19136/60032)\n",
      "[Epoch 17] Training Loss = 0.8339237119619709 (25536/60032)\n",
      "[Epoch 17] Training Loss = 0.8383583004823428 (31936/60032)\n",
      "[Epoch 17] Training Loss = 0.8374614794186638 (38336/60032)\n",
      "[Epoch 17] Training Loss = 0.8393031516044437 (44736/60032)\n",
      "[Epoch 17] Training Loss = 0.8388108905027148 (51136/60032)\n",
      "[Epoch 17] Training Loss = 0.8387421189545259 (57536/60032)\n",
      "[Epoch 17] Test Loss = 0.9325673128389249\n",
      "[Epoch 18] Training Loss = 0.8534990061413158 (6336/60032)\n",
      "[Epoch 18] Training Loss = 0.8458195876835579 (12736/60032)\n",
      "[Epoch 18] Training Loss = 0.8568730180877507 (19136/60032)\n",
      "[Epoch 18] Training Loss = 0.8537887466282474 (25536/60032)\n",
      "[Epoch 18] Training Loss = 0.8559120103925885 (31936/60032)\n",
      "[Epoch 18] Training Loss = 0.85034755101188 (38336/60032)\n",
      "[Epoch 18] Training Loss = 0.8472073957729067 (44736/60032)\n",
      "[Epoch 18] Training Loss = 0.8442533132206365 (51136/60032)\n",
      "[Epoch 18] Training Loss = 0.8441146493488477 (57536/60032)\n",
      "[Epoch 18] Test Loss = 0.9143466903905201\n",
      "[Epoch 19] Training Loss = 0.8286059643283035 (6336/60032)\n",
      "[Epoch 19] Training Loss = 0.8200398216295481 (12736/60032)\n",
      "[Epoch 19] Training Loss = 0.8224884043369803 (19136/60032)\n",
      "[Epoch 19] Training Loss = 0.8267408460752109 (25536/60032)\n",
      "[Epoch 19] Training Loss = 0.8283108074464397 (31936/60032)\n",
      "[Epoch 19] Training Loss = 0.8294054204613617 (38336/60032)\n",
      "[Epoch 19] Training Loss = 0.831949426935125 (44736/60032)\n",
      "[Epoch 19] Training Loss = 0.8322292979205207 (51136/60032)\n",
      "[Epoch 19] Training Loss = 0.8324083099442143 (57536/60032)\n",
      "[Epoch 19] Test Loss = 0.9198100984476174\n",
      "[Epoch 20] Training Loss = 0.8605430631926565 (6336/60032)\n",
      "[Epoch 20] Training Loss = 0.8500761762635791 (12736/60032)\n",
      "[Epoch 20] Training Loss = 0.841977498204413 (19136/60032)\n",
      "[Epoch 20] Training Loss = 0.832418369768855 (25536/60032)\n",
      "[Epoch 20] Training Loss = 0.8295893775437303 (31936/60032)\n",
      "[Epoch 20] Training Loss = 0.8330975117687391 (38336/60032)\n",
      "[Epoch 20] Training Loss = 0.834270147806585 (44736/60032)\n",
      "[Epoch 20] Training Loss = 0.8350662455913869 (51136/60032)\n",
      "[Epoch 20] Training Loss = 0.8362487483276542 (57536/60032)\n",
      "[Epoch 20] Test Loss = 0.9236535771637205\n",
      "[Epoch 21] Training Loss = 0.8489811233799867 (6336/60032)\n",
      "[Epoch 21] Training Loss = 0.832336632300861 (12736/60032)\n",
      "[Epoch 21] Training Loss = 0.8396113028494411 (19136/60032)\n",
      "[Epoch 21] Training Loss = 0.8391575253846353 (25536/60032)\n",
      "[Epoch 21] Training Loss = 0.8430549695878803 (31936/60032)\n",
      "[Epoch 21] Training Loss = 0.8435790474108344 (38336/60032)\n",
      "[Epoch 21] Training Loss = 0.8454382037144362 (44736/60032)\n",
      "[Epoch 21] Training Loss = 0.8455218342651563 (51136/60032)\n",
      "[Epoch 21] Training Loss = 0.8462120605191347 (57536/60032)\n",
      "[Epoch 21] Test Loss = 0.9157671841086855\n",
      "[Epoch 22] Training Loss = 0.8585747353958361 (6336/60032)\n",
      "[Epoch 22] Training Loss = 0.8491162801507729 (12736/60032)\n",
      "[Epoch 22] Training Loss = 0.845412408328774 (19136/60032)\n",
      "[Epoch 22] Training Loss = 0.8404529899134672 (25536/60032)\n",
      "[Epoch 22] Training Loss = 0.8360850302752607 (31936/60032)\n",
      "[Epoch 22] Training Loss = 0.8320008397400876 (38336/60032)\n",
      "[Epoch 22] Training Loss = 0.8294761952156673 (44736/60032)\n",
      "[Epoch 22] Training Loss = 0.829516089268411 (51136/60032)\n",
      "[Epoch 22] Training Loss = 0.8331526722738289 (57536/60032)\n",
      "[Epoch 22] Test Loss = 0.9075467149922802\n",
      "[Epoch 23] Training Loss = 0.837555845277478 (6336/60032)\n",
      "[Epoch 23] Training Loss = 0.832560466012763 (12736/60032)\n",
      "[Epoch 23] Training Loss = 0.8410622492482431 (19136/60032)\n",
      "[Epoch 23] Training Loss = 0.8421131184973514 (25536/60032)\n",
      "[Epoch 23] Training Loss = 0.8428679048298356 (31936/60032)\n",
      "[Epoch 23] Training Loss = 0.845572950993237 (38336/60032)\n",
      "[Epoch 23] Training Loss = 0.8423931802461758 (44736/60032)\n",
      "[Epoch 23] Training Loss = 0.8424289561854734 (51136/60032)\n",
      "[Epoch 23] Training Loss = 0.8406933125054611 (57536/60032)\n",
      "[Epoch 23] Test Loss = 0.8946194303263525\n",
      "[Epoch 24] Training Loss = 0.8263191249635484 (6336/60032)\n",
      "[Epoch 24] Training Loss = 0.8316973353450622 (12736/60032)\n",
      "[Epoch 24] Training Loss = 0.8244975248108739 (19136/60032)\n",
      "[Epoch 24] Training Loss = 0.8314601622876667 (25536/60032)\n",
      "[Epoch 24] Training Loss = 0.8330449894936625 (31936/60032)\n",
      "[Epoch 24] Training Loss = 0.8376650943580971 (38336/60032)\n",
      "[Epoch 24] Training Loss = 0.835182669677789 (44736/60032)\n",
      "[Epoch 24] Training Loss = 0.8358228196936645 (51136/60032)\n",
      "[Epoch 24] Training Loss = 0.8368652397321249 (57536/60032)\n",
      "[Epoch 24] Test Loss = 0.9141948340804713\n",
      "[Epoch 25] Training Loss = 0.8457401102841503 (6336/60032)\n",
      "[Epoch 25] Training Loss = 0.8488082848302084 (12736/60032)\n",
      "[Epoch 25] Training Loss = 0.8417309726959088 (19136/60032)\n",
      "[Epoch 25] Training Loss = 0.8488743915444329 (25536/60032)\n",
      "[Epoch 25] Training Loss = 0.8434540173692072 (31936/60032)\n",
      "[Epoch 25] Training Loss = 0.84327063203256 (38336/60032)\n",
      "[Epoch 25] Training Loss = 0.8426295656588285 (44736/60032)\n",
      "[Epoch 25] Training Loss = 0.83885179118758 (51136/60032)\n",
      "[Epoch 25] Training Loss = 0.8350918385489233 (57536/60032)\n",
      "[Epoch 25] Test Loss = 0.9083050654572287\n",
      "[Epoch 26] Training Loss = 0.8163117372026347 (6336/60032)\n",
      "[Epoch 26] Training Loss = 0.8188584631112352 (12736/60032)\n",
      "[Epoch 26] Training Loss = 0.8282454644756573 (19136/60032)\n",
      "[Epoch 26] Training Loss = 0.835849137010431 (25536/60032)\n",
      "[Epoch 26] Training Loss = 0.8340882139358826 (31936/60032)\n",
      "[Epoch 26] Training Loss = 0.8339160184231346 (38336/60032)\n",
      "[Epoch 26] Training Loss = 0.833014867251182 (44736/60032)\n",
      "[Epoch 26] Training Loss = 0.8331494605138394 (51136/60032)\n",
      "[Epoch 26] Training Loss = 0.8338561490327285 (57536/60032)\n",
      "[Epoch 26] Test Loss = 0.9588296451386372\n",
      "[Epoch 27] Training Loss = 0.8524261249436272 (6336/60032)\n",
      "[Epoch 27] Training Loss = 0.8430362017310444 (12736/60032)\n",
      "[Epoch 27] Training Loss = 0.836630187505065 (19136/60032)\n",
      "[Epoch 27] Training Loss = 0.8312021437145415 (25536/60032)\n",
      "[Epoch 27] Training Loss = 0.8316415476655673 (31936/60032)\n",
      "[Epoch 27] Training Loss = 0.8336569759403923 (38336/60032)\n",
      "[Epoch 27] Training Loss = 0.8354389982162116 (44736/60032)\n",
      "[Epoch 27] Training Loss = 0.8346742842164595 (51136/60032)\n",
      "[Epoch 27] Training Loss = 0.8341857683539258 (57536/60032)\n",
      "[Epoch 27] Test Loss = 0.899082083610972\n",
      "[Epoch 28] Training Loss = 0.8179442587524953 (6336/60032)\n",
      "[Epoch 28] Training Loss = 0.8216617961025717 (12736/60032)\n",
      "[Epoch 28] Training Loss = 0.8184602172877079 (19136/60032)\n",
      "[Epoch 28] Training Loss = 0.8239312463236931 (25536/60032)\n",
      "[Epoch 28] Training Loss = 0.8284722495532943 (31936/60032)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Training Loss = 0.8284840712165196 (38336/60032)\n",
      "[Epoch 28] Training Loss = 0.8274065548599363 (44736/60032)\n",
      "[Epoch 28] Training Loss = 0.8292362578446337 (51136/60032)\n",
      "[Epoch 28] Training Loss = 0.8300969023195337 (57536/60032)\n",
      "[Epoch 28] Test Loss = 0.9414943251640174\n",
      "[Epoch 29] Training Loss = 0.8288101403400151 (6336/60032)\n",
      "[Epoch 29] Training Loss = 0.8344938440538531 (12736/60032)\n",
      "[Epoch 29] Training Loss = 0.8353463303683992 (19136/60032)\n",
      "[Epoch 29] Training Loss = 0.8403156136809137 (25536/60032)\n",
      "[Epoch 29] Training Loss = 0.8436919720712788 (31936/60032)\n",
      "[Epoch 29] Training Loss = 0.8384263053760306 (38336/60032)\n",
      "[Epoch 29] Training Loss = 0.8364632703545097 (44736/60032)\n",
      "[Epoch 29] Training Loss = 0.8328086657354024 (51136/60032)\n",
      "[Epoch 29] Training Loss = 0.8342618236486056 (57536/60032)\n",
      "[Epoch 29] Test Loss = 0.8960056274559847\n"
     ]
    }
   ],
   "source": [
    "## TODO: Train your model with dropout, and monitor the training progress with the validation loss and accuracy\n",
    "model = Model(Network())\n",
    "model.fit(trainloader, testloader, epochs=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4532627131149268\n",
      "Accuracy: 0.871417224407196\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You'll also want to turn off autograd with the `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADNCAYAAADt/OSdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAH7RJREFUeJzt3XmYXFW57/HvrzN1SIAEowiYECY9gChDEPCCRAFF4BgU9IAM4j0aZ1Gc4ahc5CBOiIqKXEQEQRlEZZTRBDhAoIMIikTCGOaQhBACmbrf+8defSmqViXV6arq3cnv8zz9dNW71961qpKn315rr36XIgIzM7Oy6RjoDpiZmeU4QZmZWSk5QZmZWSk5QZmZWSk5QZmZWSk5QZmZWSk5QZlZqUg6XtJvBrofq0PS2ZJOXM1zV/q+Jf1D0uTqtpImSHpB0pDV6nSJOUGZWdtJ+qCkrvSD9UlJV0nafYD6EpIWp748LumUMv6wj4htI2JaJv5oRIyOiG4ASdMkfaTtHWwBJygzaytJxwCnAicBGwITgJ8BUwawW2+OiNHAXsAHgY9WN5A0tO29Wss5QZlZ20haHzgB+FREXBIRiyNieURcFhFfqnPORZKekrRQ0o2Stq04tp+keyUtSqOfL6b4OEmXS3pO0nxJN0la5c+7iLgPuAl4Y7rOw5K+IuluYLGkoZK2TqOU59K023uqLjNO0rWpT9MlbVrR3x9JmiPpeUkzJe1RdW6npAvSuXdKenPFuQ9L2jvz+UxMo8Chkv4b2AM4LY0IT5P0U0k/qDrnMkmfW9XnMdCcoMysnXYDOoE/9OGcq4CtgNcAdwLnVRz7JfCxiFiXIqnckOJfAB4DXk0xSjsWWGVdN0nbUPyA/2tF+FBgf2AMIOAy4JrUn88A50l6Q0X7w4BvAeOAu6r6ewewPbABcD5wkaTOiuNTgIsqjv9R0rBV9btXRBxHkWA/nab9Pg38Gji0N0FLGkcxUvxto9cdKE5QZtZOrwKejYgVjZ4QEWdFxKKIWAocD7w5jcQAlgPbSFovIhZExJ0V8Y2ATdMI7aZYeeHROyUtoEg+ZwK/qjj244iYExEvAbsCo4GTI2JZRNwAXE6RxHpdERE3pv4eB+wmaXx6L7+JiHkRsSIifgCMACqT28yIuDgilgOnUCTzXRv9rHIi4nZgIUVSAjgEmBYRT/fnuu3gBGVm7TSPYgqsofs5koZIOlnSA5KeBx5Oh8al7wcB+wGPpOm03VL8e8Bs4BpJD0r66ipeaseIGBsRW0TEf0VET8WxORWPNwbmVB1/BNgk1z4iXgDmp/OQ9AVJ/0zTlc8B61e8l+pzeyhGgRuvou+N+DVweHp8OHBuE67Zck5QZtZOtwJLgAMbbP9BimmvvSl+mE9McQFExB0RMYViuu2PwIUpvigivhARmwP/DhwjaS9WT+XI6wlgfNX9rAnA4xXPx/c+kDSaYrruiXS/6SvAB4CxETGGYmSjOud2AK9Lr7m6/e31G2BKuqe1NcVnVXpOUGbWNhGxEPgG8FNJB0paR9IwSe+W9N3MKesCSylGXutQrPwDQNJwSYdJWj9NiT0P9C61PkDSlpJUEe9uwluYASwGvpz6PZkiAf6uos1+knaXNJziXtSMiJiT3ssKYC4wVNI3gPWqrr+TpPelEebn0nu/rY99fBrYvDIQEY9R3P86F/h9mq4sPScoM2uriDgFOAb4L4of1nOAT5P/rf4ciim0x4F7qf1hfQTwcJr++zgvT2NtBVwHvEAxavtZ7m+IVqPvy4D3AO8GnqVYHn9kWv3X63zgmxRTeztRLJoAuJpiwce/0ntawiunDwH+BPwHsCC9t/el5NsXPwIOlrRA0o8r4r8GtmOQTO8ByBsWmpmt+SS9jWKqb2LVPbTS8gjKzGwNl5aqHw2cOViSEzhBmZmt0SRtDTxHsez+1AHuTp94is/MzEqprbWl9ul4/xqfDZ+97PXZ+DGvvz4bP/fx/N/grTN0WU2sc0j+bxtH1ImPGlJ7DYC3rPtATazrhc2ybWdN6uv92XK7tucirbqVmZWBix+arYXGjRsXEydOHOhu2Fpq5syZz0bEq1fVzgnKbC00ceJEurq6BrobtpaS9Egj7bxIwszMSskJyszMSslTfE02c6cLs/Hbl+YXG/x8iwuy8VEdtffyl9RZcbkk8vf9F/Xkq/Qvidp/9iM3fjbbdr/XH5SNd/+rdqGFmVkzeQRlZmal5ARlZmal5ARlZmal5ARlZmal5EUS/TDn62+tiT20/OZs23uWbNmna3d21C6qGKZ8xYgldRZDjOrIV5LI6Y7nsvE5UzbMxjf+nhdJmFlreQRlZmal5ARl1iBJt0j62iraTJR0cVVssqTvN/ga90uaJulWST9YjT5O7es5ZmXlBGXWAEnjKXZB3avFL7UwIiZHxG7A9pI26eP5TlC2xnCCMmvMwRS7kT4oaQsAScdLOk/SVZJulLROb2NJHZJ+IemwyotI2lfSTWk0dmi9F5M0BBgGLJE0VNL5kqZLulLSBqnNDyXdnEZcm0n6BPCG9HzPzDWnSuqS1DV37tymfChmreQEZdaYvYBrgN9SJKtesyLi3cBNwN4pNgQ4E7g2Is7rbSipA/hGutbuwMdTIqq0vqRpwN+BRyJiHvBe4NGI2BO4APiMpJ2BjSJid+CbwDci4uepP5MjYnr1G4iIMyJiUkRMevWrV1lI2mzAeRVfP4yf/GhNbGHdFXVLs/Hcaj2A7qj93aGbfEmj4erOxjcZuiAbv3vp+JrYAyuezLZdvGn+2msTSa8D3gRcRvFL3UjgO+nwX9P3OcDY9HgX4I6IeMW9KGAcsBVFout9/mrgqYo2CyNicnrdn0raA9gCuCMdn0GRCKtjJ67+OzQrJ4+gzFbtYODoiNg3It4JzJLUu8NjZYHE3t8gbgH+IunbVdd5FvgnsE9KQttHxFPU9xywATAb2DnFdgHurxOr7o/ZoOYRlNmqHQRMqXh+A6+c5qsREadK+rqkYykSFhHRI+m/gesk9QBzgQ9Undo7xUc6/n+AHuB9km4EFgOHRcR8SU9KuhlYAXw4nTNL0u+B70XEbav5fs1KwQnKbBUiYo+q5+dn2pxe8fTgFPtWRWxail0NXL2S19qqzqEPZtp+PhM7vN61zQYbT/GZmVkpOUGZmVkpeYqvHz4xYVpNLLcZYBHPr+7rJL+KL9e+U/m2nXVWCD7f05mNv9gzPBPL93vUJouycTOzVvMIyszMSskJyszMSskJyszMSskJyqzEUnX0uam+XpekQwa6T2bt4gRlVn7TU+WJtwFfHuC+mLWNV/H1w94jn62JzVg6Ktt2WJ16ecuiulZoIbdib3mdtut2vJSNL+oZmY2PGfJiTWxezzqZlrDVq/JVrxdno9Zi6wAvStoH+BowGrgkIk6WNAa4kKLqxOPAnIg4fsB6atYEHkGZld+eqfzR3cCvgP+JiHdQ1OA7UNJI4KPAxRGxL5Ct/OvtNmywcYIyK7/eKb6JwFHADpKuoyiftDnwGorq5jNT+ztqroC327DBxwnKbJCIiGUUFdNPBD4LvB14NMUeAHZITXcakA6aNZnvQZmVX+8U3wjgcop7TBcA9/Dy7cAzgYskvR94BrhvAPpp1lROUA3Q0PzHNLqjtpTQvO7R2bZD6OnTa+ZKHXVHfsPCemWUhij/mrnNEJfXKdG0YGl+8URtsSRrhYh4mGJTw2pnVz5Ju/W+KyK6JZ1IsV+U2aDmBGW2ZhgJ/FmSgKeBEwa4P2b95gRltgaIiMXAHqtsaDaIeJGEmZmVkhOUmZmVkhOUmZmVku9BNUAjRmTjC7prSwbB+k15zVxZo3rlkuqt1iMaf70xHbn3Ak8vXDcbH9/4pc3MVotHUGZmVkpOUGZNJmk9SZelLTJul/Tv/bzeZEnfb1b/zAYLT/GZNd8RwJ8j4qfp75KaM+/bB5I6IqJvfx1uVjIeQZk134vAWyRtGIXnJP1T0nmS/irpCABJm0u6Oo20fphi20m6QdItkk6rvKikTkkXS3pHevyb1PbSNGqbKOkmSRcBX6zulKuZ22DjBGXWfOcCs4CrU6LZCngt8AmKP6b9ZGr3HeCTqVL5UEmTKEoU7RURbwU2TudCsRfU+cCpEXED8BHghrTtxq+BqandxsBhEfHd6k65mrkNNp7ia8QW+TVrY4fU1qnrydS5W5nhdVbm0bG0JrT9iCeyTRf15GvxzevOb56YW/X3xuG1GyQCdM/Kr+Kz+iJiBXAScJKkt1OUHXowIp4HSNN+AG8AfpmergtcT1H89RRJ6wCbUSQcgCkUmxPenJ5vA+ws6UhgGHBTiv8tVT03G/ScoMyaTNKmwJMpUTxDMVORW/Q/C/hiRDySktYQ4IfATyLiSkmXUGylAfBbYIikj0fE6RTVym+NiHPTaw4DNoE+ViU2KzFP8Zk133bAjWmLjJ8C36rT7ivA6ZJuAK6lGC1dBnxP0u8pElalY4AdJR0OnAHsk+5B3QC8s/lvw2xgeQRl1mQRcTnFvk2VJlUc3zV9fxB4d1W7R4FtM5edlr5PrYgdmWl3cF/6alZmHkGZmVkpOUGZmVkpeYqvAS9OWK/hts/3jMzGxwxZnI2/dsjCbPycef+rJvbjeXtl2978pkuy8QtfyO+Gm9tRd6Tye+RueEedVYZmZi3mEZSZmZWSE5SZmZWSE5SZmZWSE5RZieQqoUvqyrT7qqTNMvGjpDo3FM0GGS+SaMDy0Y3n8VGZEkUA81eMzsYPGrUgGz/s5u1rYus8Uacfb8qHF3XnF2x0dtSWNRqi/LVH3zQ7G/fSiZZpqBJ6RJxcHZPUARwFXAy43JENeh5BmZVLTSV0YFSmEvrZkt6Y9oq6XNKlwNeB7YGrJB09gO/BrCk8gjIrl3OBjSgqob8IfIiXK6H3UJREOrfqnPWAPSMiUnHaAyLiheoLS5pKqkQxYcKE1r0DsybxCMqsRCJiRUScFBHbA8dRUQk9JR1lTuuKiFwx2upre7sNG1ScoMxKRNKmFYscVlYJvVJlBfPl1BaZNRuUnKDMyqXRSuj1XApcKOk/m90xs3bzPagGrOjMzar0zcLufNmheqvntvz8bTWxOce9tU+vWW9FYW4V37Pd+VJM3fPm9+k1rX/6UAn9qIrj0yqO/wT4Set6aNY+HkGZmVkpOUGZmVkpOUGZmVkpOUGZmVkpOUGZmVkpeRVfA5at2/gqviU9w7LxPUfdl43PXNp4Vbslr1/ScFuA7j78/tG1dIM+XdvMrNU8gjIzs1JygjJrk9xWGqt5nY9LOmolx2u25zAbjDzFZ9Y+DW2lYWYFj6DM2qdmKw1Jv0kjqpslTQCQdKekn0uaIelrKTYhtbkSeFuKdUi6Jp1/raT1VvbikqZK6pLUNXfu3Fa/V7N+c4Iya59zgVkUW2ncImkrYGpETAa+C3wstRsDnAzsBhySYl8GToiI/UjFYyOiB5iSzr8M+I+Vvbirmdtg4ym+Biwb03jb5ZEvJL1rZz6+zc8+k42P55aa2NgNarb4AeDJFfn48nhNNr5ux0s1sT/Mn5RpCZCv52d9FxErgJOAk9K+TScAz0raHhgB/CM1XRARjwBI6v3H2hKYmR7fno6NAn6RRl5jgN+35Y2YtYlHUGZtktlKYxywYUTsAZzIy3s95bbXmA3skB73/jaxL/BERLwNOJP8XlFmg5ZHUGbtsx1wgaTeP2g7GjhN0rXAvas497vA+ZK+CDyXYrcBx0m6AngSeKwFfTYbME5QZm1SZyuNPTLtcttrPArsnrnsjis732ww8xSfmZmVkkdQDViyZb7EUHf01MRymwECLOypXZgAMP7E2sUQ9Qwf2nhZJKhfdmlI5hbH3fM3zrYdyUN9ek0zs2bxCMrMzErJCcrMzErJCcrMzErJCcrMzErJCcqsZCTtkerr3SjpeklvbPC8MZI+0Or+mbWLV/E1YFjnimy8J/sH/3lDmvBH/gsXj8zG663t61C+f92Zvsx57FXZtq/3Kr62kvQq4GfAPhHxVHqeX2JZawzwAeDCVvXPrJ08gjIrl/2BSyLiKYCImAc8mvaRmi7pQknDJW0o6bo0yrpY0hDgE8CeafT1hoF8E2bN4ARlVi4bAU9UxaYCV0TEnhQFZQ8FFgD7pjp8jwLvAH4OTI+IyRExq/rC3m7DBhsnKLNyeQLYpCq2BXBHejyDorL5BsDFkqYDB9DANKC327DBxgnKrFyuAN4r6bUAkjagKAK7czq+C3A/cBhwTRpVXU5RyXw5kN/XxWwQcoIyK5GImA98kqLq+XTgAorNCA+QdCOwLfA74HrgE5L+BLw2nf4kMDLdk9q8/b03ay6v4mvA8qfzq+fuXla7fm5Rd77t6I7ObLxj+22y8Z67andfeGnuOtm2ncqvEOyJfLxTtfUCh87N1+2z9ouIm4A9q8IHVD2/i2L7jmr7tqRTZgPAIygzMyslJygzMyslJygzMyslJygzMyslJygzMyslr+JrwLoP5f+0ZKcRw2tit7yYv8ZdS5dm44/tMyYb3/iu2tiIZ/L/XMOU/z2jXi2+Yapdfdg5v/+1As3MmskjKDMzKyWPoMwGkKSJFGWM7qGoBnEjcGJE1P6xmtlaxiMos4E3PSLeQVHwtQP4XO8Bqc78rdlawCMos5KIiJB0InC9pEOBW4D1JX0UOJOiIOwLwOHAa4BzgaXAvyJiqqSzKQrLBnBkRDzc/ndh1jxOUA3Y5Jz78ge+VBvaYOgL2aZLIv9Rv7D1sob7MbTOAozl0ZOND1d+o8Wckc80vvmitU5ELJU0AhgLnBoRsyV9GrghIs6SdBDF9hsLgPMi4jRJHZKGAVsDu6ZEVzPykjQ1ncuECRPa9p7MVpenD8xKRNJwYBmwICJmp/A2FIVhpwHHAOMods19naRzgMPTPasfAWdJOhWoKdzo7TZssPEIyqxcjgX+RLF1e6/7gFsj4lyANFoaGhFfTc/vlXQecFFEnC/pWOB9wDnt7bpZczlBmQ28PSXdQDGjcTNwKq9MUGcAZ0j6cHr+A2B0mvobAfwZWBe4NE3t9QCHtKvzZq3iBGU2gNJChtx826SKNkuAIzNtLqh6/rbm9cxs4PkelJmZlZJHUA3onjc/G3+2e3FNbFTH6Pw1yJcS2vn1D2XjCzOxzvn5lXbLo/8r8EY8n18JaGY2UDyCMjOzUnKCMjOzUnKCMjOzUnKCMjOzUnKCMjOzUvIqvn749jN71MSmjL0z23Z5nVp8G4/MrdfLr+IbvqjOKr589+paHrUbMA5Z4lV8zVSxjcY/gGHAhypKF1W264qISZKOB7oi4vJ29tOszDyCMmud6RExGTgF+Eo7X9jbdNiawP+JzVrv78Dhkr4PIOnf0tYYWZJ+KOlmSdMkbSbp/ZK+nI6tJ+na9PhYSdMl3ShpuxS7U9JpwK8z150qqUtS19y5c1vwNs2aywnKrPX2AGY10lDSzsBGEbE78E3gG8DlwP6pyYHAH1NCekNE7ElRt++EdLx3m44jqq/tauY22DhBmbXOnmmLjP2Aoyvi+bIihS0o7l0BzAC2jIiXgMclbQkcBFxMsffTW9P1fwesl85ZkLvXZTYYeZGEWetMj4iDASS9CRif4jut5JzZFKMkgF2A+9PjC4CPUWyz8bSk+9L1P5KuPyy182oXW2M4QfXD5VfuUhP70BG3ZNsu6hmejy/vrHP12p15R89ZUufatavyAIapu+G4/GOt1e4BOiVdR5GEsiKiS9KTkm4GVgC9W2z8GTgL+HJqd7ek+yVNp0hK1wIntfINmLWbE5RZC6RtNA6ueB7AlEy7Sen78RWxz2faLQVeVRX7DvCd3PXM1gS+B2VmZqXkBGVmZqXkBGVmZqXke1D9sPHNmSJDNX99Uqi3YOHRxWOz8Y7MIolhj83Ltl1cp4zSMK3Ixp/rXqcmNvKhBdm2+V7bYHfP4wuZ+NUrBrobNsg9fPL+q27UDx5BmZlZKTlBmZlZKXmKz6xFJA0HrklPdwJmpscHRETtHK6ZvYITlFmLRMQyYDL8/201Jlcel9QRES35E2lJSn3I79FiNgh4is+sjSTtLelSSZcCH5S0j6TbJM2QdERq8xtJ/5Yef1/S7unr9lTh/Jvp2P6pkvktkj5Qce7PgOuBdQfobZo1hUdQ/dA5/e81sfuWbZhtu+3wp7LxsSNezMZzGxbG4nzbnsj/ntGp/FaG9y7dpCbWPcv1RdtoNLBXRISkO4B3AYuBGZIurHPO/sA3IuLPkjokDQGOpRih9QB/kXRRajsjIj5ZfQFJU4GpAEPWczVzKz+PoMzar6ti6i0iYn4qZTQbeC1QOS3XW/n8J8B+ks4B3glsCGxFUYPveooySL2lkO4go3K7jSHrrN/UN2TWCh5BmbVf5X0nSdqAojrwlsBTwAJgvKRZwI7AHyi20fispE6KbTh2BO4D9omI5ZKGpe/V1zcbtJygzAbWccBV6fEPImKppLModsR9GOgtYf9JSVOAUcCvIqJb0snAdZJ6KBLboe3tullrOUGZtUFF1fLrgOsq4tfw8lL03thdwJurLnEz8IOqdlcCV1bFDm9er80Glu9BmZlZKXkE1Q89S2o3ELzw6Z2zbU+f+KdsfOcxD2fj12VWCHc/m6/F9/iKfD2/Vw99Pht/obveJom2tthuk/XpanEdNbP+8gjKzMxKyQnKzMxKyQnKzMxKyQnKzMxKyQnKzMxKyav4GtExJB/vqd1vduY/N8s2Hbv5yGx8wfJRq92tXgeNzq/Wu3tZ7SpDgIUrcn1x0etGrM4WGqmS+aSq2FHArIi4tSp+IHBLRDyTnm8N/G/gfyrjZmsDJyizPljVFhp9uM7Z1TFJHcCBFDX5ehPRvhSVJo6siput8TzFZ9ZEknZL22JMl3RCCndI+nnaUuNrqd3xkg6QNFHSTakS+ZcoEtKvJH07nbs78HhlXNL6ki5Lr3GhpOGSJku6In3dLmmrTN+mSuqS1DV37tzWfxhm/eQRlFlz7QecEBGXpxERwBjgZGAO8Ffg21XnbEyx/cayNKX3/Yj4u6SRwPKImCXpzxXxLwFXRMTpaW+oQ4FHgPWBPYDdKGr8HVX5IhFxBnAGwKRJkzyna6XnEZRZP0k6Jm0keAzwU2CftC3GvqnJgoh4JO2e+1LmEn9LU4fVJgPTM/EteHlLjRkUVdAB/pq28ZiZ2pgNah5BNUAdysZzm3VvdXbu5ww8uW9+s8HPjbs1Gz9y0sdqX6+rdoNEgNuW1C7WANh06Ips/P5Fr8lEn862tVWLiFOAUwAkjYyIo9NiipkUxVxXNVqp/J+0HOhdlfMu4EeZ+Gxg53T9XYD7U3z7tNX7DsADq/2GzErCCcqsuT4m6X0U22KcvRrnXwWcKulqYLOIeCgT/zlwnqQPUmyz8W3grcAi4ApgHHBYv96FWQk4QZmtpuql4yl2KnBqvXYRsWv6fnxFk4Mrjl8CXCJpBMWGhK+IV5xzQOVrpI0K742IL67GWzErJScosxJKW8D/caD7YTaQnKDM1gARMQ2YNsDdMGsqr+IzM7NS8giqyXTL37Lx78+dnI1fNXubbHxi190Nv+Zht3wkG7/v7Wdm4/fMrC3HtKVX8ZlZyXgEZWZmpeQEZWZmpeQEZWZmpeR7UGZroZkzZ74gadZA96PCOODZge5ElbL1qWz9gdXv06aNNHKCMls7zcr9ofFAye2ZNdDK1qey9Qda3ycVtSXNbG1Sth92ZesPlK9PZesPtL5PvgdlZmal5ARltnY6Y6A7UKVs/YHy9als/YEW98lTfGZmVkoeQZmZWSk5QZmZWSk5QZmtYSTtK2mWpNmSvpo5PkLSBen4DEkTK459LcVnSXpXm/pzjKR7Jd0t6XpJm1Yc65Z0V/q6tE39OUrS3IrX/UjFsQ9Juj99fagZ/WmwTz+s6M+/JD1XcawVn9FZkp6RlN3GW4Ufp/7eLWnHimPN+4wiwl/+8tca8kWxLfwDwObAcOBvwDZVbT4JnJ4eHwJckB5vk9qPADZL1xnShv68HVgnPf5Eb3/S8xcG4PM5Cjgtc+4GwIPp+9j0eGw7+lTV/jPAWa36jNI13wbsCPy9zvH9KHZ5FrArMKMVn5FHUGZrlrcAsyPiwYhYBvwOmFLVZgrw6/T4YmAvFVvyTgF+FxFLo9hqfna6Xkv7ExF/iYgX09PbgNf18zX71Z+VeBdwbUTMj4gFwLXAvgPQp0OB3zbhdeuKiBuB+StpMgU4Jwq3AWMkbUSTPyMnKLM1yybAnIrnj6VYtk1ErAAWAq9q8NxW9KfSf1L8Zt6rU1KXpNskHdjPvvSlPwelqauLJY3v47mt6hNp+nMz4IaKcLM/o0bU63NTPyOXOjJbsygTq/5bknptGjm3Ff0pGkqHA5OAPSvCEyLiCUmbAzdIuiciHmhxfy4DfhsRSyV9nGK0+Y4Gz21Vn3odAlwcEd0VsWZ/Ro1oy/8hj6DM1iyPAeMrnr8OeKJeG0lDgfUppnMaObcV/UHS3sBxwHsiYmlvPCKeSN8fpNjSfodW9yci5lX04f8COzV6bqv6VOEQqqb3WvAZNaJen5v7GTX75pq//OWvgfuimBV5kGIaqPeG+7ZVbT7FKxdJXJgeb8srF0k8SP8XSTTSnx0oFglsVRUfC4xIj8cB97OSxQNN7M9GFY/fC9yWHm8APJT6NTY93qAd/2ap3RuAh0kFFlr1GVVceyL1F0nszysXSdzeis/IU3xma5CIWCHp08DVFKvDzoqIf0g6AeiKiEuBXwLnSppNMXI6JJ37D0kXAvcCK4BPxSunklrVn+8Bo4GLirUaPBoR7wG2Bn4hqYditufkiLi3Df35rKT3UHwG8ylW9RER8yV9C7gjXe6EiFjZQoJm9gmKxRG/i5QJkqZ/RgCSfgtMBsZJegz4JjAs9fd04EqKlXyzgReBD6djTf2MXOrIzMxKyfegzMyslJygzMyslJygzMyslJygzMyslJygzMyslJygzMyslJygzMyslJygzMyslJygzMyslP4fOAwTHJSW5WgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import helper module (should be in the repo)\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "# with torch.no_grad():\n",
    "#     output = model.forward(img)\n",
    "\n",
    "# ps = torch.exp(output)\n",
    "\n",
    "ps, pred = model.predict(img)\n",
    "ps = ps.to('cpu')\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next part, I'll show you how to save your trained models. In general, you won't want to train a model everytime you need it. Instead, you'll train once, save it, then load the model when you want to train more or use if for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
